{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 5: Full Code Implementation for Exercises\n",
    "\n",
    "Sarah Marvin\n",
    "\n",
    "### Exercise 5.1:"
   ],
   "id": "168927c92f1c8fcc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T02:58:50.631511Z",
     "start_time": "2025-04-25T02:58:50.536443Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "# Define a small vocabulary\n",
    "vocab = {\n",
    "    \"closer\": 0, \"every\": 1, \"effort\": 2, \"forward\": 3,\n",
    "    \"inches\": 4, \"moves\": 5, \"pizza\": 6, \"toward\": 7, \"you\": 8\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Example logits for the next token prediction\n",
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "\n",
    "# Temperature-scaled softmax function\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    \"\"\"\n",
    "    Applies temperature scaling to the logits and computes softmax.\n",
    "    \"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Sampling function to print sampled tokens based on probabilities\n",
    "def print_sampled_tokens(probas):\n",
    "    \"\"\"\n",
    "    Sample tokens from the probability distribution and print their frequencies.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(123)  # Ensure reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "# Set of temperatures to experiment with\n",
    "temperatures = [1, 0.1, 5]  # Normal, lower randomness, higher randomness\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "# Print sampled tokens for each temperature\n",
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(f\"Temperature: {temperatures[i]}\")\n",
    "    print_sampled_tokens(probas)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Check the actual probability of \"pizza\" at temperature 5\n",
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "pizza_prob = scaled_probas[temp5_idx][pizza_idx]\n",
    "print(f\"Actual probability of 'pizza': {pizza_prob:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1\n",
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n",
      "\n",
      "\n",
      "Actual probability of 'pizza': 0.0430\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 5.2:",
   "id": "f5448cb4975bfcac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T02:58:54.972230Z",
     "start_time": "2025-04-25T02:58:54.932517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Top-k sampling function\n",
    "def apply_top_k_sampling(logits, top_k):\n",
    "    \"\"\"\n",
    "    Applies top-k sampling to logits to limit the options to the top-k probable tokens.\n",
    "    \"\"\"\n",
    "    values, indices = torch.topk(logits, top_k)\n",
    "    probs = torch.softmax(values, dim=0)\n",
    "    return torch.zeros_like(logits).scatter_(0, indices, probs)\n",
    "\n",
    "# Apply top-k sampling and temperature scaling\n",
    "logits = apply_top_k_sampling(next_token_logits, top_k=3)  # Using top-3 tokens\n",
    "logits = softmax_with_temperature(logits, temperature=1.0)\n",
    "print_sampled_tokens(logits)\n"
   ],
   "id": "68e47a608263a827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 x closer\n",
      "85 x every\n",
      "117 x effort\n",
      "169 x forward\n",
      "106 x inches\n",
      "94 x moves\n",
      "93 x pizza\n",
      "144 x toward\n",
      "81 x you\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 5.3:\n",
   "id": "6dda4feb491ea4d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T02:59:31.929566Z",
     "start_time": "2025-04-25T02:59:31.815384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Dummy logits for demonstration (replace with your actual logits)\n",
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "\n",
    "# Softmax with temperature and clipping for numerical stability\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    \"\"\"\n",
    "    Apply temperature scaling to logits and compute softmax with numerical stability.\n",
    "    \"\"\"\n",
    "    logits = logits / temperature if temperature != 0 else logits\n",
    "    # Clip logits to avoid extreme values\n",
    "    logits = torch.clamp(logits, min=-100, max=100)\n",
    "    return torch.softmax(logits, dim=-1)\n",
    "\n",
    "def apply_top_k_sampling(logits, top_k=1):\n",
    "    \"\"\"\n",
    "    Apply top-k sampling to logits to keep only the top-k most probable tokens.\n",
    "    \"\"\"\n",
    "    if top_k > 1:\n",
    "        values, indices = torch.topk(logits, top_k)\n",
    "        logits = torch.zeros_like(logits).scatter_(-1, indices, values)\n",
    "    return logits\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    \"\"\"\n",
    "    Sample tokens from the probability distribution and print their frequencies.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(123)  # Ensure reproducibility\n",
    "    if torch.any(torch.isnan(probas)) or torch.any(torch.isinf(probas)):\n",
    "        print(\"Invalid probability values found!\")\n",
    "        return\n",
    "\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x Token {i}\")\n",
    "\n",
    "# Set top_k to 1 and temperature to 0 for deterministic output\n",
    "def generate_deterministic_text(logits):\n",
    "    \"\"\"\n",
    "    Generate deterministic text by setting top_k=1 and temperature=0.\n",
    "    \"\"\"\n",
    "    logits = apply_top_k_sampling(logits, top_k=1)  # Choose the most probable token\n",
    "    logits = softmax_with_temperature(logits, temperature=0.0)  # Set temperature to 0 for deterministic behavior\n",
    "    print_sampled_tokens(logits)\n",
    "\n",
    "# Call the function with the sample logits\n",
    "generate_deterministic_text(next_token_logits)"
   ],
   "id": "d9b8d6e7213bacb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x Token 0\n",
      "2 x Token 1\n",
      "0 x Token 2\n",
      "544 x Token 3\n",
      "2 x Token 4\n",
      "1 x Token 5\n",
      "0 x Token 6\n",
      "376 x Token 7\n",
      "4 x Token 8\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 5.4:",
   "id": "db3c88513f2484a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define GPT model (simplified version)\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, seq_length):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # Embedding layer\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=8, num_encoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)  # Output layer\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the GPT model.\n",
    "        Args:\n",
    "            src: Source sequence (input tokens)\n",
    "            tgt: Target sequence (output tokens, for training purposes)\n",
    "        \"\"\"\n",
    "        # Embedding source and target sequences\n",
    "        src = self.embed(src)\n",
    "        if tgt is not None:\n",
    "            tgt = self.embed(tgt)\n",
    "\n",
    "        # Ensure the shape (seq_len, batch_size, embed_dim) for transformer\n",
    "        src = src.permute(1, 0, 2)  # (batch_size, seq_len, embed_dim) -> (seq_len, batch_size, embed_dim)\n",
    "        if tgt is not None:\n",
    "            tgt = tgt.permute(1, 0, 2)  # Same for target\n",
    "\n",
    "        # Pass through the transformer (encoder-decoder)\n",
    "        if tgt is not None:\n",
    "            output = self.transformer(src, tgt)  # Training mode, both src and tgt provided\n",
    "        else:\n",
    "            output = self.transformer(src)  # Generation mode, only src is provided\n",
    "\n",
    "        # Convert back to (batch_size, seq_len, hidden_size)\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        # Output layer to map to vocab size\n",
    "        logits = self.fc_out(output)\n",
    "        return logits\n",
    "\n",
    "    def generate_text(self, input_ids, max_length=50):\n",
    "        \"\"\"\n",
    "        Generate text given an initial sequence of tokens (input_ids).\n",
    "        \"\"\"\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        generated = input_ids\n",
    "        for _ in range(max_length):\n",
    "            logits = self(generated, generated)  # Using the generated text as input and target\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "            generated = torch.cat((generated, next_token), dim=-1)\n",
    "        return generated\n",
    "\n",
    "\n",
    "# Create mock input and output tensors for model training\n",
    "input_tensor = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 5]], dtype=torch.long)\n",
    "output_tensor = torch.tensor([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]], dtype=torch.long)  # Dummy target tensor\n",
    "\n",
    "# Model training loop\n",
    "model = GPTModel(vocab_size=9, embed_size=128, hidden_size=128, num_layers=2, seq_length=5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor, output_tensor)  # Pass both source and target\n",
    "    loss = criterion(output.view(-1, 9), output_tensor.view(-1))  # Flatten and compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ],
   "id": "b679c00646117f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 5.5:",
   "id": "b6b9a868e4c9f3da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer from Hugging Face\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set pad_token_id explicitly, as GPT2 does not have a pad token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Update model config with pad_token_id\n",
    "\n",
    "# Tokenize input text\n",
    "start_context = \"Every effort moves you\"\n",
    "input_ids = tokenizer.encode(start_context, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n",
    "\n",
    "# Generate the attention mask (1 for actual tokens, 0 for padding tokens)\n",
    "attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=50)  # Generate a sequence of tokens\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)  # Decode back to text\n",
    "\n",
    "print(generated_text)"
   ],
   "id": "3d1a4167366ef320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exercise 5.6:",
   "id": "9ba6d070490622bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "input_ids = tokenizer.encode(start_context, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n",
    "attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
    "\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ],
   "id": "3d47dac082ec6136",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
